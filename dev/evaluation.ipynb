{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd46daf-a409-490f-945f-804ccebabdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, BertPreTrainedModel, RobertaPreTrainedModel, DebertaV2PreTrainedModel, AutoModel, AutoModelForSequenceClassification, BertConfig, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import nltk\n",
    "nltk.data.path.append(\"./nltk_data\")\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb442a89-fd9a-4eb2-9895-56f23f7e7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress false warnings from transformers\n",
    "import logging\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Suppress the specific sklearn warning about class imbalance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.utils.multiclass')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08663f2d-2d3d-43c3-bf4f-36adcefd77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of benchmark files\n",
    "BENCHMARK_FILES = [\n",
    "    \"senseval2.parquet\",\n",
    "    \"senseval3.parquet\",\n",
    "    \"semeval2007.parquet\",\n",
    "    \"semeval2013.parquet\",\n",
    "    \"semeval2015.parquet\",\n",
    "    \"ALL.parquet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6d1923-5e5c-4510-acb1-26967908bf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e2d0c-d4f1-450e-8f53-d564fd5e40e8",
   "metadata": {},
   "source": [
    "> ##### For each of the trained models, I will write code to load and calculate Accuracy and F1 score metrics on all test datasets, taking into account the specifics of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7b1e4-f63d-4f02-a0bf-cc4464ffff97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BERT (WSDInferenceDataset class + evaluation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d385273b-52d2-477f-8378-14db169bb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_PATH = \"./models/bert/bert_wsd_custom/\"  \n",
    "LABEL_MAP_FILE = \"label_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278fde39-3e8b-43f0-a018-7071ad7b155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Model Class Definition (BERT) ---\n",
    "class BertForWSD(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = AutoModel.from_config(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, target_token_idx=None, labels=None, **kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        batch_indices = torch.arange(batch_size, device=input_ids.device)\n",
    "        target_vectors = sequence_output[batch_indices, target_token_idx]\n",
    "\n",
    "        target_vectors = self.dropout(target_vectors)\n",
    "        logits = self.classifier(target_vectors)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cf9faa-217b-4923-bd04-93e14d6da3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Inference Dataset ---\n",
    "class WSDInferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        sentence = row['sentence']\n",
    "        c_start = row['char_start']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Find target token index\n",
    "        offsets = encoding['offset_mapping'].squeeze().tolist()\n",
    "        target_token_idx = 0\n",
    "        \n",
    "        for i, (o_start, o_end) in enumerate(offsets):\n",
    "            if o_start == 0 and o_end == 0: continue\n",
    "            if o_start == c_start:\n",
    "                target_token_idx = i\n",
    "                break\n",
    "            if o_start < c_start and o_end > c_start:\n",
    "                target_token_idx = i\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'target_token_idx': torch.tensor(target_token_idx, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ac911a-ca21-4474-ae66-016bb2628e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Helper Functions ---\n",
    "def load_resources():\n",
    "    \"\"\"Loads tokenizer, label map and the model.\"\"\"\n",
    "    print(\"Loading resources...\")\n",
    "    \n",
    "    # Load Label Map\n",
    "    with open(LABEL_MAP_FILE, 'r') as f:\n",
    "        label2id = json.load(f)\n",
    "    # Create reverse map: ID -> Label Name (e.g., 0 -> 'art.n.01')\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    \n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "    # Load Model\n",
    "    model = BertForWSD.from_pretrained(MODEL_PATH, num_labels=len(label2id))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, id2label, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb69c21-9c99-457c-b328-231f6110f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(file_path, model, tokenizer, id2label):\n",
    "    \"\"\"Runs prediction loop for a single dataset and calculates metrics.\"\"\"\n",
    "\n",
    "    file_path = os.path.join(\"./evaluation_data/parquet/\", file_path)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: File {file_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    dataset = WSDInferenceDataset(df, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    all_preds_ids = []\n",
    "    \n",
    "    # Inference Loop\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            target_idx = batch['target_token_idx'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                target_token_idx=target_idx\n",
    "            )\n",
    "            \n",
    "            # Get predicted class ID\n",
    "            logits = outputs['logits']\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds_ids.extend(preds)\n",
    "            \n",
    "    # --- Metrics Calculation ---\n",
    "    y_true_for_sklearn = []\n",
    "    y_pred_for_sklearn = []\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_count = len(df)\n",
    "    \n",
    "    for idx, pred_id in enumerate(all_preds_ids):\n",
    "        # Convert ID to String Label\n",
    "        pred_label = id2label.get(pred_id, \"UNK\")\n",
    "        \n",
    "        # Get Gold Labels (List of strings)\n",
    "        gold_labels = df.iloc[idx]['gold_synsets'] # Expecting a list/array\n",
    "        \n",
    "        # Correct if prediction is IN the gold list\n",
    "        if pred_label in gold_labels:\n",
    "            correct_count += 1\n",
    "            # For sklearn metrics, if correct, we say Expected == Predicted\n",
    "            y_true_for_sklearn.append(pred_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "        else:\n",
    "            # If incorrect, we set Expected = First Gold Label\n",
    "            # This allows us to calculate F1\n",
    "            target_label = gold_labels[0] if len(gold_labels) > 0 else \"UNK\"\n",
    "            y_true_for_sklearn.append(target_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "            \n",
    "    # Calculate Metrics\n",
    "    acc = accuracy_score(y_true_for_sklearn, y_pred_for_sklearn)\n",
    "    micro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='micro')\n",
    "    macro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"Dataset\": os.path.basename(file_path).replace('.parquet', ''),\n",
    "        \"Accuracy\": acc,\n",
    "        \"Micro F1\": micro_f1,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"Samples\": total_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "044ae272-bf61-4a1e-ac79-0b1149f888c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resources...\n",
      "\n",
      "Starting Benchmarking...\n",
      "------------------------------------------------------------\n",
      "Processing senseval2.parquet...\n",
      "   -> Accuracy: 0.6109\n",
      "Processing senseval3.parquet...\n",
      "   -> Accuracy: 0.6216\n",
      "Processing semeval2007.parquet...\n",
      "   -> Accuracy: 0.5978\n",
      "Processing semeval2013.parquet...\n",
      "   -> Accuracy: 0.5237\n",
      "Processing semeval2015.parquet...\n",
      "   -> Accuracy: 0.6057\n",
      "Processing ALL.parquet...\n",
      "   -> Accuracy: 0.5923\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Final Benchmark Results ===\n",
      "    Dataset  Accuracy  Micro F1  Macro F1  Samples\n",
      "  senseval2    0.6109    0.6109    0.3988     2282\n",
      "  senseval3    0.6216    0.6216    0.3920     1850\n",
      "semeval2007    0.5978    0.5978    0.4014      455\n",
      "semeval2013    0.5237    0.5237    0.3075     1644\n",
      "semeval2015    0.6057    0.6057    0.3794     1022\n",
      "        ALL    0.5923    0.5923    0.3483     7253\n",
      "\n",
      "Results saved to 'bert_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Main Execution ---\n",
    "tokenizer, id2label, model = load_resources()\n",
    "    \n",
    "results = []\n",
    "\n",
    "print(\"\\nStarting Benchmarking...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for filename in BENCHMARK_FILES:\n",
    "    print(f\"Processing {filename}...\")\n",
    "    metrics = evaluate_dataset(filename, model, tokenizer, id2label)\n",
    "    if metrics:\n",
    "        results.append(metrics)\n",
    "        print(f\"   -> Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        \n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create Final DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Formatting for display\n",
    "print(\"\\n=== Final Benchmark Results ===\")\n",
    "print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"./results/bert_benchmark_results.csv\", index=False)\n",
    "print(\"\\nResults saved to 'bert_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432a9d0-da9d-4951-a14f-4d6b0a825858",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f34c71-de96-4bae-9d67-0a13ab9b9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_PATH = \"./models/roberta/roberta_wsd_custom/\" \n",
    "LABEL_MAP_FILE = \"label_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18530d7e-a489-420a-8104-9583d8d307a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Model Class Definition (RoBERTa) ---\n",
    "class RobertaForWSD(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = AutoModel.from_config(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, target_token_idx=None, labels=None, **kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        # 1. Run RoBERTa\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        # 2. Extract specific vector\n",
    "        batch_size = input_ids.shape[0]\n",
    "        batch_indices = torch.arange(batch_size, device=input_ids.device)\n",
    "        \n",
    "        # Gather embeddings for the target token index\n",
    "        target_vectors = sequence_output[batch_indices, target_token_idx]\n",
    "\n",
    "        # 3. Classification\n",
    "        target_vectors = self.dropout(target_vectors)\n",
    "        logits = self.classifier(target_vectors)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad2a738-9501-4aeb-84b8-d97f90172e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Inference Dataset (Same as for BERT) ---\n",
    "# --- 3. Helper Functions ---\n",
    "def load_resources():\n",
    "    \"\"\"Loads tokenizer, label map and the RoBERTa model.\"\"\"\n",
    "    print(\"Loading resources for RoBERTa...\")\n",
    "    \n",
    "    # Load Label Map\n",
    "    with open(LABEL_MAP_FILE, 'r') as f:\n",
    "        label2id = json.load(f)\n",
    "    # Create reverse map: ID -> Label Name\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    \n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, add_prefix_space=True)\n",
    "    \n",
    "    # Load Model\n",
    "    model = RobertaForWSD.from_pretrained(MODEL_PATH, num_labels=len(label2id))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, id2label, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad5bcc76-400b-443b-b5f5-72e00990793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resources for RoBERTa...\n",
      "\n",
      "Starting Benchmarking...\n",
      "------------------------------------------------------------\n",
      "Processing senseval2.parquet...\n",
      "   -> Accuracy: 0.6078\n",
      "Processing senseval3.parquet...\n",
      "   -> Accuracy: 0.6162\n",
      "Processing semeval2007.parquet...\n",
      "   -> Accuracy: 0.6000\n",
      "Processing semeval2013.parquet...\n",
      "   -> Accuracy: 0.5207\n",
      "Processing semeval2015.parquet...\n",
      "   -> Accuracy: 0.6057\n",
      "Processing ALL.parquet...\n",
      "   -> Accuracy: 0.5894\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Final Benchmark Results ===\n",
      "    Dataset  Accuracy  Micro F1  Macro F1  Samples\n",
      "  senseval2    0.6078    0.6078    0.3976     2282\n",
      "  senseval3    0.6162    0.6162    0.3811     1850\n",
      "semeval2007    0.6000    0.6000    0.4019      455\n",
      "semeval2013    0.5207    0.5207    0.3149     1644\n",
      "semeval2015    0.6057    0.6057    0.3774     1022\n",
      "        ALL    0.5894    0.5894    0.3426     7253\n",
      "\n",
      "Results saved to 'roberta_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Main Execution ---\n",
    "tokenizer, id2label, model = load_resources()\n",
    "    \n",
    "results = []\n",
    "\n",
    "print(\"\\nStarting Benchmarking...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for filename in BENCHMARK_FILES:\n",
    "    print(f\"Processing {filename}...\")\n",
    "    metrics = evaluate_dataset(filename, model, tokenizer, id2label)\n",
    "    if metrics:\n",
    "        results.append(metrics)\n",
    "        print(f\"   -> Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        \n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create Final DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Formatting for display\n",
    "print(\"\\n=== Final Benchmark Results ===\")\n",
    "print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"./results/roberta_benchmark_results.csv\", index=False)\n",
    "print(\"\\nResults saved to 'roberta_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a98da3-4958-4e2f-abb5-13b96e62d996",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8694f134-edad-4d75-861e-d7c299a00b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_PATH = \"./models/deberta/deberta_wsd_custom/\" \n",
    "LABEL_MAP_FILE = \"label_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ef794d-8390-4825-b496-1beec40abfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Model Class Definition (DeBERTa V3) ---\n",
    "class DebertaV3ForWSD(DebertaV2PreTrainedModel): \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # AutoModel handles loading the correct backbone (DebertaV2Model)\n",
    "        self.deberta = AutoModel.from_config(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, target_token_idx=None, labels=None, **kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        batch_indices = torch.arange(batch_size, device=input_ids.device)\n",
    "        \n",
    "        # Extract vector for the specific target token\n",
    "        target_vectors = sequence_output[batch_indices, target_token_idx]\n",
    "\n",
    "        target_vectors = self.dropout(target_vectors)\n",
    "        logits = self.classifier(target_vectors)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # Return dictionary to match evaluation loop expectations\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c2eec3-960f-4284-8d02-d227a78fd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Inference Dataset (The same one) ---\n",
    "# --- 3. Helper Functions (Updated) ---\n",
    "def load_resources():\n",
    "    \"\"\"Loads tokenizer, label map and the DeBERTa model.\"\"\"\n",
    "    print(\"Loading resources for DeBERTa...\")\n",
    "    \n",
    "    # Load Label Map\n",
    "    with open(LABEL_MAP_FILE, 'r') as f:\n",
    "        label2id = json.load(f)\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    \n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "    # Load Model\n",
    "    model = DebertaV3ForWSD.from_pretrained(MODEL_PATH, num_labels=len(label2id))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, id2label, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c921392-d4c1-4c15-b705-0bda825c9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resources for DeBERTa...\n",
      "\n",
      "Starting Benchmarking...\n",
      "------------------------------------------------------------\n",
      "Processing senseval2.parquet...\n",
      "   -> Accuracy: 0.6074\n",
      "Processing senseval3.parquet...\n",
      "   -> Accuracy: 0.6168\n",
      "Processing semeval2007.parquet...\n",
      "   -> Accuracy: 0.6000\n",
      "Processing semeval2013.parquet...\n",
      "   -> Accuracy: 0.5182\n",
      "Processing semeval2015.parquet...\n",
      "   -> Accuracy: 0.5959\n",
      "Processing ALL.parquet...\n",
      "   -> Accuracy: 0.5875\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Final Benchmark Results ===\n",
      "    Dataset  Accuracy  Micro F1  Macro F1  Samples\n",
      "  senseval2    0.6074    0.6074    0.3902     2282\n",
      "  senseval3    0.6168    0.6168    0.3845     1850\n",
      "semeval2007    0.6000    0.6000    0.3941      455\n",
      "semeval2013    0.5182    0.5182    0.3039     1644\n",
      "semeval2015    0.5959    0.5959    0.3635     1022\n",
      "        ALL    0.5875    0.5875    0.3367     7253\n",
      "\n",
      "Results saved to 'deberta_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Main Execution ---\n",
    "tokenizer, id2label, model = load_resources()\n",
    "    \n",
    "results = []\n",
    "\n",
    "print(\"\\nStarting Benchmarking...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for filename in BENCHMARK_FILES:\n",
    "    print(f\"Processing {filename}...\")\n",
    "    metrics = evaluate_dataset(filename, model, tokenizer, id2label)\n",
    "    if metrics:\n",
    "        results.append(metrics)\n",
    "        print(f\"   -> Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        \n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create Final DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Formatting for display\n",
    "print(\"\\n=== Final Benchmark Results ===\")\n",
    "print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"./results/deberta_benchmark_results.csv\", index=False)\n",
    "print(\"\\nResults saved to 'deberta_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0463ed-5fc6-4726-acb7-73c8edab0510",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeBERTa - Logit Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f2fc259-2032-440b-a43b-8c9b730e9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_PATH = \"./models/deberta_masked/deberta_wsd_masked/\" \n",
    "LABEL_MAP_FILE = \"label_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c07f50-1bf4-4deb-b162-2bbf28a57d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Dataset Class with Logit Masking ---\n",
    "class WSDDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, label2id, max_len=128):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        sentence = row['sentence']\n",
    "        target_word = row['target_word']\n",
    "        c_start = row['char_start']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        offsets = encoding['offset_mapping'].squeeze().tolist()\n",
    "        target_token_idx = 0\n",
    "\n",
    "        # Find target token\n",
    "        for i, (o_start, o_end) in enumerate(offsets):\n",
    "            if o_start == 0 and o_end == 0: continue\n",
    "            if o_start == c_start:\n",
    "                target_token_idx = i\n",
    "                break\n",
    "            if o_start < c_start and o_end > c_start:\n",
    "                 target_token_idx = i\n",
    "                 break\n",
    "        \n",
    "        # --- Logit Masking Logic ---\n",
    "        # 1. Initialize mask with large negative value\n",
    "        mask = torch.full((self.num_labels,), -1e4, dtype=torch.float32)\n",
    "        \n",
    "        # 2. Get candidate synsets from NLTK based purely on the target word\n",
    "        lookup_word = target_word.replace(\" \", \"_\")\n",
    "        synsets = wn.synsets(lookup_word)\n",
    "        \n",
    "        found_candidates = False\n",
    "\n",
    "        # 3. Activate valid indices found in NLTK\n",
    "        for synset in synsets:\n",
    "            s_name = synset.name()\n",
    "            if s_name in self.label2id:\n",
    "                mask[self.label2id[s_name]] = 0.0\n",
    "                found_candidates = True\n",
    "        \n",
    "        # 4. Fallback: If NLTK finds NOTHING known, unmask ALL labels.\n",
    "        if not found_candidates:\n",
    "            mask = torch.zeros((self.num_labels,), dtype=torch.float32)\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'target_token_idx': torch.tensor(target_token_idx, dtype=torch.long),\n",
    "            'logit_mask': mask\n",
    "        }\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34b7ce0-1e84-428b-b12b-46448410694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Model Class ---\n",
    "class DebertaV3ForWSD(DebertaV2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.deberta = AutoModel.from_config(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, target_token_idx=None, labels=None, logit_mask=None, **kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        batch_indices = torch.arange(batch_size, device=input_ids.device)\n",
    "\n",
    "        target_vectors = sequence_output[batch_indices, target_token_idx]\n",
    "        target_vectors = self.dropout(target_vectors)\n",
    "        logits = self.classifier(target_vectors)\n",
    "\n",
    "        # Apply Logit Masking \n",
    "        if logit_mask is not None:\n",
    "            logits = logits + logit_mask\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e7eed2d-b9e9-4c70-a3d6-314c4f4bbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Helper Functions ---\n",
    "def load_resources():\n",
    "    print(\"Loading resources for DeBERTa Masked...\")\n",
    "    \n",
    "    with open(LABEL_MAP_FILE, 'r') as f:\n",
    "        label2id = json.load(f)\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "    model = DebertaV3ForWSD.from_pretrained(MODEL_PATH, num_labels=len(label2id))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, id2label, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abe903e6-efb0-454c-8325-d83b2082755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(file_path, model, tokenizer, id2label):\n",
    "    \"\"\"Runs prediction loop using Logit Masking.\"\"\"\n",
    "    \n",
    "    file_path = os.path.join(\"./evaluation_data/parquet/\", file_path)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: File {file_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_parquet(file_path)\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    dataset = WSDDataset(df, tokenizer, label2id)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    all_preds_ids = []\n",
    "    \n",
    "    # Inference Loop\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            target_idx = batch['target_token_idx'].to(device)\n",
    "            logit_mask = batch['logit_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                target_token_idx=target_idx,\n",
    "                logit_mask=logit_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs['logits']\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds_ids.extend(preds)\n",
    "            \n",
    "    # --- Metrics Calculation ---\n",
    "    y_true_for_sklearn = []\n",
    "    y_pred_for_sklearn = []\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_count = len(df)\n",
    "    \n",
    "    for idx, pred_id in enumerate(all_preds_ids):\n",
    "        pred_label = id2label.get(pred_id, \"UNK\")\n",
    "        \n",
    "        gold_labels = df.iloc[idx]['gold_synsets'] \n",
    "        \n",
    "        if pred_label in gold_labels:\n",
    "            correct_count += 1\n",
    "            y_true_for_sklearn.append(pred_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "        else:\n",
    "            target_label = gold_labels[0] if len(gold_labels) > 0 else \"UNK\"\n",
    "            y_true_for_sklearn.append(target_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "            \n",
    "    acc = accuracy_score(y_true_for_sklearn, y_pred_for_sklearn)\n",
    "    micro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='micro')\n",
    "    macro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"Dataset\": os.path.basename(file_path).replace('.parquet', ''),\n",
    "        \"Accuracy\": acc,\n",
    "        \"Micro F1\": micro_f1,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"Samples\": total_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93e25a6f-cb01-4779-bb9c-a0e58897f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resources for DeBERTa Masked...\n",
      "\n",
      "Starting Benchmarking with Logit Masking...\n",
      "------------------------------------------------------------\n",
      "Processing senseval2.parquet...\n",
      "   -> Accuracy: 0.6494\n",
      "Processing senseval3.parquet...\n",
      "   -> Accuracy: 0.6730\n",
      "Processing semeval2007.parquet...\n",
      "   -> Accuracy: 0.6264\n",
      "Processing semeval2013.parquet...\n",
      "   -> Accuracy: 0.6223\n",
      "Processing semeval2015.parquet...\n",
      "   -> Accuracy: 0.6614\n",
      "Processing ALL.parquet...\n",
      "   -> Accuracy: 0.6495\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Final Benchmark Results ===\n",
      "    Dataset  Accuracy  Micro F1  Macro F1  Samples\n",
      "  senseval2    0.6494    0.6494    0.4598     2282\n",
      "  senseval3    0.6730    0.6730    0.4570     1850\n",
      "semeval2007    0.6264    0.6264    0.4365      455\n",
      "semeval2013    0.6223    0.6223    0.4177     1644\n",
      "semeval2015    0.6614    0.6614    0.4304     1022\n",
      "        ALL    0.6495    0.6495    0.4313     7253\n",
      "\n",
      "Results saved to './results/deberta_masked_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "tokenizer, id2label, model = load_resources()\n",
    "    \n",
    "results = []\n",
    "print(\"\\nStarting Benchmarking with Logit Masking...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for filename in BENCHMARK_FILES:\n",
    "    print(f\"Processing {filename}...\")\n",
    "    metrics = evaluate_dataset(filename, model, tokenizer, id2label)\n",
    "    if metrics:\n",
    "        results.append(metrics)\n",
    "        print(f\"   -> Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        \n",
    "print(\"-\" * 60)\n",
    "\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n=== Final Benchmark Results ===\")\n",
    "    print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "    \n",
    "    output_path = \"./results/deberta_masked_benchmark_results.csv\"\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nResults saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77003e42-3236-458e-b477-61a95c2a9cd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gloss - DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4c92fcf-092c-4292-bc80-70c5e49f10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_PATH = \"./models/deberta_gloss/deberta_wsd_gloss/\" \n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d151b961-6c55-47a8-94ef-0da04f27dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Dataset Class ---\n",
    "class GlossInferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=256):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        context = row['context']\n",
    "        gloss = row['gloss']\n",
    "        \n",
    "        # Prepare input as pairs: [CLS] Context [SEP] Gloss [SEP]\n",
    "        encoding = self.tokenizer(\n",
    "            context,\n",
    "            gloss,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask']\n",
    "        }\n",
    "        \n",
    "        if 'token_type_ids' in encoding:\n",
    "            item['token_type_ids'] = encoding['token_type_ids']\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f516668-0aa3-455c-93e8-c5898b9c0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Evaluation Function ---\n",
    "def evaluate_gloss_model(original_filename, model, tokenizer, data_collator):\n",
    "    \"\"\"\n",
    "    Runs prediction pipeline:\n",
    "    1. Loads gloss_dataset (candidates)\n",
    "    2. Predicts scores for all candidates\n",
    "    3. Selects best candidate per instance\n",
    "    4. Compares with ORIGINAL dataset (ground truth)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    original_path = os.path.join(\"evaluation_data/parquet\", original_filename)\n",
    "    gloss_filename = original_filename.replace(\".parquet\", \"_gloss.parquet\")\n",
    "    gloss_path = os.path.join(\"evaluation_data/gloss\", gloss_filename)\n",
    "\n",
    "    if not os.path.exists(original_path):\n",
    "        print(f\"Warning: Original file {original_path} not found.\")\n",
    "        return None\n",
    "    if not os.path.exists(gloss_path):\n",
    "        print(f\"Warning: Gloss file {gloss_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    # Load Dataframes\n",
    "    df_original = pd.read_parquet(original_path)\n",
    "    df_gloss = pd.read_parquet(gloss_path)\n",
    "    \n",
    "    # Create Dataset\n",
    "    dataset = GlossInferenceDataset(df_gloss, tokenizer)\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    all_scores = []\n",
    "    \n",
    "    # --- Inference Loop ---\n",
    "    # We only need the score for the \"Positive\" class (label 1)\n",
    "    print(f\"Running inference on {len(df_gloss)} pairs...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Inference\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "            inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "            if 'token_type_ids' in batch:\n",
    "                inputs['token_type_ids'] = batch['token_type_ids'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            # We take the probability of class 1 (Entailment/Correct Gloss)\n",
    "            positive_scores = probs[:, 1].cpu().numpy()\n",
    "            all_scores.extend(positive_scores)\n",
    "            \n",
    "    # Add scores back to the dataframe\n",
    "    df_gloss['score'] = all_scores\n",
    "    \n",
    "    # --- Aggregation (Select Best Candidate) ---\n",
    "    # For each instance_group_id, find the row with the max score\n",
    "    best_candidates_idx = df_gloss.groupby('instance_group_id')['score'].idxmax()\n",
    "    \n",
    "    # Extract just the predictions: dictionary {instance_id: predicted_synset}\n",
    "    best_predictions = df_gloss.loc[best_candidates_idx][['instance_group_id', 'candidate_synset']]\n",
    "    prediction_map = dict(zip(best_predictions['instance_group_id'], best_predictions['candidate_synset']))\n",
    "    \n",
    "    # --- Final Scoring against Original Ground Truth ---\n",
    "    y_true_for_sklearn = []\n",
    "    y_pred_for_sklearn = []\n",
    "    \n",
    "    # Iterate over the ORIGINAL dataset to ensure full coverage\n",
    "    for idx, row in df_original.iterrows():\n",
    "        inst_id = row['id']\n",
    "        gold_synsets = row['gold_synsets']\n",
    "        \n",
    "        # Ensure gold is a list\n",
    "        if isinstance(gold_synsets, np.ndarray): gold_synsets = gold_synsets.tolist()\n",
    "        elif not isinstance(gold_synsets, list): gold_synsets = [gold_synsets]\n",
    "        \n",
    "        # Get model prediction\n",
    "        pred_label = prediction_map.get(inst_id, \"UNK\")\n",
    "        \n",
    "        # Check correctness\n",
    "        if pred_label in gold_synsets:\n",
    "            y_true_for_sklearn.append(pred_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "        else:\n",
    "            # For F1 calc, align expected with one of the valid golds\n",
    "            target_label = gold_synsets[0] if len(gold_synsets) > 0 else \"UNK\"\n",
    "            y_true_for_sklearn.append(target_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "            \n",
    "    # Calculate Metrics\n",
    "    acc = accuracy_score(y_true_for_sklearn, y_pred_for_sklearn)\n",
    "    micro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='micro')\n",
    "    macro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"Dataset\": original_filename.replace('.parquet', ''),\n",
    "        \"Accuracy\": acc,\n",
    "        \"Micro F1\": micro_f1,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"Samples\": len(df_original)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "557868ac-9a13-4638-92e7-0a06595e8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resources():\n",
    "    print(\"Loading Gloss DeBERTa resources...\")\n",
    "    \n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "    # Ensure [TGT] token exists\n",
    "    if '[TGT]' not in tokenizer.additional_special_tokens:\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n",
    "    \n",
    "    # Load Model (Binary Classification)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    return tokenizer, model, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de8383b8-ae01-4229-99ab-d13431b209dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gloss DeBERTa resources...\n",
      "\n",
      "Starting Gloss DeBERTa Benchmarking...\n",
      "------------------------------------------------------------\n",
      "Processing senseval2.parquet...\n",
      "Running inference on 16736 pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1feef7c5fd47fdade703aa9328f3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Accuracy: 0.7401\n",
      "Processing senseval3.parquet...\n",
      "Running inference on 16789 pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3293cd16b604480e962ce1657461eead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Accuracy: 0.7103\n",
      "Processing semeval2007.parquet...\n",
      "Running inference on 4634 pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a25f45f07134b98ba5762567a522cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Accuracy: 0.6857\n",
      "Processing semeval2013.parquet...\n",
      "Running inference on 11451 pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1d37594b564604aadb38177ae2709d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Accuracy: 0.7749\n",
      "Processing semeval2015.parquet...\n",
      "Running inference on 7582 pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94dcfe1b4cb493bbff98785e1e382f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Accuracy: 0.7808\n",
      "Processing ALL.parquet...\n",
      "Running inference on 57192 pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95db4c6a0b5c4a479d1b56240b834225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Accuracy: 0.7427\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Final Benchmark Results (Gloss DeBERTa) ===\n",
      "    Dataset  Accuracy  Micro F1  Macro F1  Samples\n",
      "  senseval2    0.7401    0.7401    0.5278     2282\n",
      "  senseval3    0.7103    0.7103    0.5290     1850\n",
      "semeval2007    0.6857    0.6857    0.5345      455\n",
      "semeval2013    0.7749    0.7749    0.5969     1644\n",
      "semeval2015    0.7808    0.7808    0.5963     1022\n",
      "        ALL    0.7427    0.7427    0.5503     7253\n",
      "\n",
      "Results saved to 'deberta_gloss_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model, data_collator = load_resources()\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"\\nStarting Gloss DeBERTa Benchmarking...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for filename in BENCHMARK_FILES:\n",
    "    print(f\"Processing {filename}...\")\n",
    "    metrics = evaluate_gloss_model(filename, model, tokenizer, data_collator)\n",
    "    if metrics:\n",
    "        results.append(metrics)\n",
    "        print(f\"   -> Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        \n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create Final DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Formatting for display\n",
    "print(\"\\n=== Final Benchmark Results (Gloss DeBERTa) ===\")\n",
    "print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "# Save to CSV\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "results_df.to_csv(\"./results/deberta_gloss_benchmark_results.csv\", index=False)\n",
    "print(\"\\nResults saved to 'deberta_gloss_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a07989-5a1f-434b-bb41-ec81030a29f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lesk & Most Frequent Sense (MFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e95cd9-a6d0-44e5-869d-34b475b510ae",
   "metadata": {},
   "source": [
    "> I will write functions to obtain predictions from MFS, MFS with automatic part-of-speech accounting, and Lesk algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e768025-d454-4609-bf26-8ffd742158dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Maps NLTK POS tags (Treebank) to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed6fac7-3b3c-4a49-85b1-7b06d4328c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mfs_pos(sentence, target_word):\n",
    "    \"\"\"\n",
    "    Predicts MFS considering POS and LEMMATIZATION.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize and Tag\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # 2. Find the POS tag for the target word\n",
    "    wn_pos = None\n",
    "    for word, tag in tagged_tokens:\n",
    "        if word == target_word or word.lower() == target_word.lower():\n",
    "            wn_pos = get_wordnet_pos(tag)\n",
    "            break\n",
    "            \n",
    "    # 3. Lemmatize and Query WordNet\n",
    "    if wn_pos:\n",
    "        # Lemmatize with the specific POS\n",
    "        lemma = lemmatizer.lemmatize(target_word.lower(), pos=wn_pos)\n",
    "        synsets = wn.synsets(lemma, pos=wn_pos)\n",
    "    else:\n",
    "        # Fallback: Lemmatize as Noun (default) and query without POS restriction\n",
    "        lemma = lemmatizer.lemmatize(target_word.lower())\n",
    "        synsets = wn.synsets(lemma)\n",
    "\n",
    "    # 4. Final Fallback mechanism\n",
    "    # If finding by specific POS failed, try finding by lemma only\n",
    "    if not synsets and wn_pos:\n",
    "         synsets = wn.synsets(lemma)\n",
    "         \n",
    "    if not synsets:\n",
    "        # Last resort: try original word\n",
    "        synsets = wn.synsets(target_word)\n",
    "\n",
    "    if not synsets:\n",
    "        return \"UNK\"\n",
    "        \n",
    "    return synsets[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4938887b-b0f8-4903-a8f0-30ecdcdf74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mfs(target_word):\n",
    "    \"\"\"\n",
    "    Predicts the Most Frequent Sense (MFS).\n",
    "    WordNet returns synsets ordered by frequency usage.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(target_word)\n",
    "    if not synsets:\n",
    "        return \"UNK\"\n",
    "    # Take the first one as it is the most frequent\n",
    "    return synsets[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9073f26b-411b-40c5-b6e5-477f6f749ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lesk(sentence, target_word):\n",
    "    \"\"\"\n",
    "    Predicts sense using the Lesk algorithm (gloss overlap).\n",
    "    \"\"\"\n",
    "    # Tokenization is required for the Lesk algorithm\n",
    "    sent_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # nltk.wsd.lesk returns a Synset object or None\n",
    "    synset = lesk(sent_tokens, target_word)\n",
    "    \n",
    "    if synset:\n",
    "        return synset.name()\n",
    "    return \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fcab50f-df25-49a0-9aed-b9cb018dd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_for_baseline(df, method_name):\n",
    "    \"\"\"\n",
    "    Calculates metrics for a specific baseline method (MFS, MFS_POS, or Lesk).\n",
    "    Replicates the logic used in the transformer evaluation for consistency.\n",
    "    \"\"\"\n",
    "    y_true_for_sklearn = []\n",
    "    y_pred_for_sklearn = []\n",
    "    \n",
    "    correct_count = 0\n",
    "    \n",
    "    # Iterate through the dataset\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        target_word = row['target_word']\n",
    "        gold_labels = row['gold_synsets']\n",
    "        \n",
    "        # 1. Prediction Step\n",
    "        if method_name == \"Lesk\":\n",
    "            pred_label = predict_lesk(sentence, target_word)\n",
    "        elif method_name == \"MFS\":\n",
    "            pred_label = predict_mfs(target_word)\n",
    "        elif method_name == \"MFS_POS\":\n",
    "            pred_label = predict_mfs_pos(sentence, target_word)\n",
    "        else:\n",
    "            pred_label = \"UNK\"\n",
    "            \n",
    "        # 2. Evaluation Logic\n",
    "        if pred_label in gold_labels:\n",
    "            correct_count += 1\n",
    "            y_true_for_sklearn.append(pred_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "        else:\n",
    "            target_label = gold_labels[0] if len(gold_labels) > 0 else \"UNK\"\n",
    "            y_true_for_sklearn.append(target_label)\n",
    "            y_pred_for_sklearn.append(pred_label)\n",
    "            \n",
    "    # Calculate Metrics\n",
    "    acc = accuracy_score(y_true_for_sklearn, y_pred_for_sklearn)\n",
    "    micro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='micro')\n",
    "    macro_f1 = f1_score(y_true_for_sklearn, y_pred_for_sklearn, average='macro')\n",
    "    \n",
    "    return acc, micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f61a1287-c419-4ef4-845c-e435bf1924ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_loop(method_name, file_list, output_filename):\n",
    "    \"\"\"\n",
    "    Iterates through all files for a specific method, prints a table, \n",
    "    and saves the results to a CSV file.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting Benchmarking for method: {method_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Dataset':<20} | {'Accuracy':<10} | {'Micro F1':<10} | {'Macro F1':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(\"./evaluation_data/parquet/\", filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: File {file_path} not found.\")\n",
    "            continue\n",
    "            \n",
    "        # Load Dataset\n",
    "        df = pd.read_parquet(file_path)\n",
    "        total_count = len(df)\n",
    "        dataset_name = filename.replace('.parquet', '')\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc, micro, macro = calculate_metrics_for_baseline(df, method_name)\n",
    "        \n",
    "        results.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Micro F1\": micro,\n",
    "            \"Macro F1\": macro,\n",
    "            \"Samples\": total_count\n",
    "        })\n",
    "        \n",
    "        # Print row to console\n",
    "        print(f\"{dataset_name:<20} | {acc:.4f}     | {micro:.4f}     | {macro:.4f}\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        save_path = os.path.join(\"./results/\", output_filename)\n",
    "        results_df.to_csv(save_path, index=False)\n",
    "        print(f\"Results for {method_name} saved to '{save_path}'\")\n",
    "        return results_df\n",
    "    else:\n",
    "        print(f\"No results generated for {method_name}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d108c926-542d-4e35-acb0-9b634379feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Benchmarking for method: Lesk\n",
      "======================================================================\n",
      "Dataset              | Accuracy   | Micro F1   | Macro F1  \n",
      "----------------------------------------------------------------------\n",
      "senseval2            | 0.3545     | 0.3545     | 0.1868\n",
      "senseval3            | 0.3097     | 0.3097     | 0.1678\n",
      "semeval2007          | 0.2110     | 0.2110     | 0.1319\n",
      "semeval2013          | 0.3631     | 0.3631     | 0.2403\n",
      "semeval2015          | 0.3542     | 0.3542     | 0.2009\n",
      "ALL                  | 0.3360     | 0.3360     | 0.1992\n",
      "----------------------------------------------------------------------\n",
      "Results for Lesk saved to './results/lesk_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# 1. Run Loop for Lesk\n",
    "lesk_df = run_benchmark_loop(\n",
    "    method_name=\"Lesk\", \n",
    "    file_list=BENCHMARK_FILES, \n",
    "    output_filename=\"lesk_benchmark_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983e9084-ae5c-4683-bacb-5c1e7b4624bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Benchmarking for method: MFS\n",
      "======================================================================\n",
      "Dataset              | Accuracy   | Micro F1   | Macro F1  \n",
      "----------------------------------------------------------------------\n",
      "senseval2            | 0.4939     | 0.4939     | 0.2824\n",
      "senseval3            | 0.4708     | 0.4708     | 0.2874\n",
      "semeval2007          | 0.3956     | 0.3956     | 0.2584\n",
      "semeval2013          | 0.5335     | 0.5335     | 0.3919\n",
      "semeval2015          | 0.4941     | 0.4941     | 0.2954\n",
      "ALL                  | 0.4908     | 0.4908     | 0.2972\n",
      "----------------------------------------------------------------------\n",
      "Results for MFS saved to './results/mfs_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# 2. Run Loop for MFS (Simple)\n",
    "mfs_df = run_benchmark_loop(\n",
    "    method_name=\"MFS\", \n",
    "    file_list=BENCHMARK_FILES, \n",
    "    output_filename=\"mfs_benchmark_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49071418-31f1-4000-8061-fb9d2e47d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Benchmarking for method: MFS_POS\n",
      "======================================================================\n",
      "Dataset              | Accuracy   | Micro F1   | Macro F1  \n",
      "----------------------------------------------------------------------\n",
      "senseval2            | 0.6091     | 0.6091     | 0.3933\n",
      "senseval3            | 0.5897     | 0.5897     | 0.3866\n",
      "semeval2007          | 0.5099     | 0.5099     | 0.3651\n",
      "semeval2013          | 0.5493     | 0.5493     | 0.4018\n",
      "semeval2015          | 0.6067     | 0.6067     | 0.4097\n",
      "ALL                  | 0.5840     | 0.5840     | 0.3766\n",
      "----------------------------------------------------------------------\n",
      "Results for MFS_POS saved to './results/mfs_pos_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# 3. Run Loop for MFS with POS (Smart) - NEW\n",
    "mfs_pos_df = run_benchmark_loop(\n",
    "    method_name=\"MFS_POS\", \n",
    "    file_list=BENCHMARK_FILES, \n",
    "    output_filename=\"mfs_pos_benchmark_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a10ce-8cc2-4602-9726-64b1920be808",
   "metadata": {},
   "source": [
    "### Final comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e8cc84b-3680-475b-a99a-c4bcb59117ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"./results/\"\n",
    "\n",
    "files = [\n",
    "    \"bert_benchmark_results.csv\",\n",
    "    \"lesk_benchmark_results.csv\",\n",
    "    \"roberta_benchmark_results.csv\",\n",
    "    \"deberta_benchmark_results.csv\",\n",
    "    \"deberta_gloss_benchmark_results.csv\",\n",
    "    \"mfs_benchmark_results.csv\",\n",
    "    \"mfs_pos_benchmark_results.csv\",\n",
    "    \"deberta_masked_benchmark_results.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be5e1609-66f0-448b-917b-b859a9d3df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "print(\"Loading files...\")\n",
    "for filename in files:\n",
    "    file_path = os.path.join(RESULTS_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = filename.replace(\"_benchmark_results.csv\", \"\").replace(\"_\", \" \").upper()\n",
    "        df['Model'] = model_name\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: File {filename} not found.\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf6b34e-33c8-4776-9cb9-674fec189920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved combined results to './results/combined_benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "cols = ['Model', 'Dataset', 'Accuracy', 'Micro F1', 'Macro F1', 'Samples']\n",
    "cols = [c for c in cols if c in full_df.columns]\n",
    "full_df = full_df[cols]\n",
    "\n",
    "full_df.to_csv(\"./results/combined_benchmark_results.csv\", index=False)\n",
    "print(\"\\nSaved combined results to './results/combined_benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "184172cd-d8c2-4032-80b4-3ef93cb93b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== All Results Combined ===\n",
      "         Model     Dataset  Accuracy  Micro F1  Macro F1  Samples\n",
      "          BERT   senseval2    0.6109    0.6109    0.3988     2282\n",
      "          BERT   senseval3    0.6216    0.6216    0.3920     1850\n",
      "          BERT semeval2007    0.5978    0.5978    0.4014      455\n",
      "          BERT semeval2013    0.5237    0.5237    0.3075     1644\n",
      "          BERT semeval2015    0.6057    0.6057    0.3794     1022\n",
      "          BERT         ALL    0.5923    0.5923    0.3483     7253\n",
      "          LESK   senseval2    0.3545    0.3545    0.1868     2282\n",
      "          LESK   senseval3    0.3097    0.3097    0.1678     1850\n",
      "          LESK semeval2007    0.2110    0.2110    0.1319      455\n",
      "          LESK semeval2013    0.3631    0.3631    0.2403     1644\n",
      "          LESK semeval2015    0.3542    0.3542    0.2009     1022\n",
      "          LESK         ALL    0.3360    0.3360    0.1992     7253\n",
      "       ROBERTA   senseval2    0.6078    0.6078    0.3976     2282\n",
      "       ROBERTA   senseval3    0.6162    0.6162    0.3811     1850\n",
      "       ROBERTA semeval2007    0.6000    0.6000    0.4019      455\n",
      "       ROBERTA semeval2013    0.5207    0.5207    0.3149     1644\n",
      "       ROBERTA semeval2015    0.6057    0.6057    0.3774     1022\n",
      "       ROBERTA         ALL    0.5894    0.5894    0.3426     7253\n",
      "       DEBERTA   senseval2    0.6074    0.6074    0.3902     2282\n",
      "       DEBERTA   senseval3    0.6168    0.6168    0.3845     1850\n",
      "       DEBERTA semeval2007    0.6000    0.6000    0.3941      455\n",
      "       DEBERTA semeval2013    0.5182    0.5182    0.3039     1644\n",
      "       DEBERTA semeval2015    0.5959    0.5959    0.3635     1022\n",
      "       DEBERTA         ALL    0.5875    0.5875    0.3367     7253\n",
      " DEBERTA GLOSS   senseval2    0.7401    0.7401    0.5278     2282\n",
      " DEBERTA GLOSS   senseval3    0.7103    0.7103    0.5290     1850\n",
      " DEBERTA GLOSS semeval2007    0.6857    0.6857    0.5345      455\n",
      " DEBERTA GLOSS semeval2013    0.7749    0.7749    0.5969     1644\n",
      " DEBERTA GLOSS semeval2015    0.7808    0.7808    0.5963     1022\n",
      " DEBERTA GLOSS         ALL    0.7427    0.7427    0.5503     7253\n",
      "           MFS   senseval2    0.4939    0.4939    0.2824     2282\n",
      "           MFS   senseval3    0.4708    0.4708    0.2874     1850\n",
      "           MFS semeval2007    0.3956    0.3956    0.2584      455\n",
      "           MFS semeval2013    0.5335    0.5335    0.3919     1644\n",
      "           MFS semeval2015    0.4941    0.4941    0.2954     1022\n",
      "           MFS         ALL    0.4908    0.4908    0.2972     7253\n",
      "       MFS POS   senseval2    0.6091    0.6091    0.3933     2282\n",
      "       MFS POS   senseval3    0.5897    0.5897    0.3866     1850\n",
      "       MFS POS semeval2007    0.5099    0.5099    0.3651      455\n",
      "       MFS POS semeval2013    0.5493    0.5493    0.4018     1644\n",
      "       MFS POS semeval2015    0.6067    0.6067    0.4097     1022\n",
      "       MFS POS         ALL    0.5840    0.5840    0.3766     7253\n",
      "DEBERTA MASKED   senseval2    0.6494    0.6494    0.4598     2282\n",
      "DEBERTA MASKED   senseval3    0.6730    0.6730    0.4570     1850\n",
      "DEBERTA MASKED semeval2007    0.6264    0.6264    0.4365      455\n",
      "DEBERTA MASKED semeval2013    0.6223    0.6223    0.4177     1644\n",
      "DEBERTA MASKED semeval2015    0.6614    0.6614    0.4304     1022\n",
      "DEBERTA MASKED         ALL    0.6495    0.6495    0.4313     7253\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== All Results Combined ===\")\n",
    "print(full_df.to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb3341d5-26ba-4222-9274-d569a4300936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison on 'ALL' Dataset ===\n",
      "         Model  Accuracy  Macro F1\n",
      "          LESK    0.3360    0.1992\n",
      "           MFS    0.4908    0.2972\n",
      "       MFS POS    0.5840    0.3766\n",
      "       DEBERTA    0.5875    0.3367\n",
      "       ROBERTA    0.5894    0.3426\n",
      "          BERT    0.5923    0.3483\n",
      "DEBERTA MASKED    0.6495    0.4313\n",
      " DEBERTA GLOSS    0.7427    0.5503\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Comparison on 'ALL' Dataset ===\")\n",
    "all_dataset_df = full_df[full_df['Dataset'] == 'ALL'].sort_values(by='Accuracy', ascending=True)\n",
    "print(all_dataset_df[['Model', 'Accuracy', 'Macro F1']].to_string(index=False, float_format=\"%.4f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
