{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59b1e2c-f057-4f48-845e-8b51a1fa1ec6",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940bf14-2fdb-4ab9-b86e-2620a43b7830",
   "metadata": {},
   "source": [
    "## Getting Started with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366dd4ee-f901-4637-9dc8-d2896b64309e",
   "metadata": {},
   "source": [
    "> I'll download and import SemCor corpus and other necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bda843-d507-4f9c-b54d-1e7cce733528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to ./nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to ./nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to ./nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ./nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     ./nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "# Create an unverified SSL context to bypass certificate verification issues\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Older Python versions may not support SSL context modification\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download the SemCor corpus and WordNet lexicon (into a local directory)\n",
    "nltk.download(\"semcor\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Add the custom directory to NLTK's search path so it can find the downloaded data | uncomment if using 'download_dir=\"./nltk_data\"' in nltk.download()\n",
    "# nltk.data.path.append(\"./nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88102e44-3d28-4efe-b225-0a7c14c966bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a158e-312d-47ac-aa10-e4efa3bd166d",
   "metadata": {},
   "source": [
    "> Get the first sentence from the SemCor corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefb5fd7-0e99-4ec0-87c7-0bd52a7e32f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n",
      "\n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced `` no evidence '' that any irregularities took place .\n"
     ]
    }
   ],
   "source": [
    "sent = semcor.sents()[0]\n",
    "print(sent, \"\\n\")\n",
    "print(\" \".join(semcor.sents()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1ec9f-38a4-4bd7-b47b-d81982091875",
   "metadata": {},
   "source": [
    "> Get the first semantically tagged sentence from the SemCor corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5cafd4-073c-4833-a825-1136157b62e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.02.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']]\n"
     ]
    }
   ],
   "source": [
    "# 'tagged_sents(tag=\"sem\")' returns sentences where each word may have a WordNet sense annotation\n",
    "# Parameter 'tag=\"sem\"' means \"semantic tags\" (WordNet senses)\n",
    "# NE = Named Entity\n",
    "tagged_sent = semcor.tagged_sents(tag=\"sem\")[0]\n",
    "\n",
    "# Print the full structure of the first tagged sentence\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f22f113-adcb-424b-badf-c48dcf3bc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The']\n",
      "(Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n",
      "(Lemma('state.v.01.say') said)\n",
      "(Lemma('friday.n.01.Friday') Friday)\n",
      "['an']\n",
      "(Lemma('probe.n.01.investigation') investigation)\n",
      "['of']\n",
      "(Lemma('atlanta.n.01.Atlanta') Atlanta)\n",
      "[\"'s\"]\n",
      "(Lemma('late.s.02.recent') recent)\n",
      "(Lemma('primary.n.01.primary_election') primary election)\n",
      "(Lemma('produce.v.04.produce') produced)\n",
      "['``']\n",
      "['no']\n",
      "(Lemma('evidence.n.01.evidence') evidence)\n",
      "[\"''\"]\n",
      "['that']\n",
      "['any']\n",
      "(Lemma('abnormality.n.04.irregularity') irregularities)\n",
      "(Lemma('happen.v.01.take_place') took place)\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "for word in tagged_sent:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90143b55-1cc2-45bd-ade3-2686aad900bb",
   "metadata": {},
   "source": [
    "> **Explanation of structure**:\n",
    "> \n",
    "> - `semcor` is a corpus of English sentences annotated with WordNet senses.\n",
    "> - Each sentence is represented as a tree-like structure of words and lemmas.  \n",
    "> - Example elements:  \n",
    ">     - `['The']` - a plain unannotated word  \n",
    ">     - `Tree(Lemma('group.n.01.group'), [...])` - word/s linked to a WordNet sense (`group.n.01`)  \n",
    "> - So each `tagged_sent` is a list of tokens, where some are annotated with `Lemma` objects that store:  \n",
    ">     - the lemma (base form),  \n",
    ">     - the WordNet sense ID (like `group.n.01`),  \n",
    ">     - and the part of speech.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6db8d-a159-488a-83e3-d4ca380eb4e4",
   "metadata": {},
   "source": [
    "> Get the sense for `happen.v.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8037eb02-9681-40be-abb2-c1f8f765f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come to pass\n",
      "['What is happening?', 'The meeting took place off without an incidence', 'Nothing occurred that seemed important']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a specific WordNet synset (a set of cognitive synonyms)\n",
    "# The format is \"word.pos.number\" â€” e.g., \"group.n.01\" means:\n",
    "#   - word = \"group\"\n",
    "#   - pos = \"n\" (noun)\n",
    "#   - 01 = the first sense of the noun \"group\"\n",
    "synset = wn.synset(\"happen.v.01\")\n",
    "\n",
    "# Print the human-readable definition (gloss) of this sense\n",
    "print(synset.definition())\n",
    "\n",
    "# Print example sentences showing how this sense is used (if there are any)\n",
    "print(synset.examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48057d04-8c96-4c95-a365-3fc4381719d8",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd48ba6-6aa3-49c7-9584-ccac0d1c7867",
   "metadata": {},
   "source": [
    "> I'll write a function to create a dataset of words and their meanings based on SemCor. Dataset will store the sentence ID, the sentence itself, the target word, the indices of its start and end characters in the sentence, the WordNet label, and the generated ID for that label. The function also saves the resulting dataset in parquet format and the dictionary of generated IDs for WordNet labels in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc36ebfb-28fa-424d-8ff0-35ccf6d5acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semcor_dataset_with_offsets(output_file=\"semcor_train.parquet\", label_map_file=\"label_map.json\"):\n",
    "    \n",
    "    print(\"Creating dataset from SemCor corpus...\")\n",
    "    \n",
    "    dataset_rows = []\n",
    "    seen_synsets = set()\n",
    "    \n",
    "    for sent_idx, chunk_sent in enumerate(semcor.tagged_sents(tag='sem')):\n",
    "        \n",
    "        # Construct the sentence dynamically, tracking the cursor position\n",
    "        full_sentence = \"\"\n",
    "        targets_in_sentence = [] \n",
    "        \n",
    "        # Current cursor position in the string (character index)\n",
    "        current_char_pos = 0\n",
    "        \n",
    "        for chunk in chunk_sent:\n",
    "            # 1. Extract chunk text\n",
    "            if isinstance(chunk, nltk.Tree):\n",
    "                original_words = chunk.leaves()\n",
    "                word_form = \" \".join(original_words) # For the case of multiple words\n",
    "                is_target = True\n",
    "                label_lemma = chunk.label()\n",
    "            else:\n",
    "                # chunk is list ['The']\n",
    "                word_form = \" \".join(chunk)\n",
    "                is_target = False\n",
    "            \n",
    "            # 2. Add a space before the word if it is not the start of the sentence\n",
    "            prefix = \"\"\n",
    "            if current_char_pos > 0:\n",
    "                prefix = \" \"\n",
    "                full_sentence += prefix\n",
    "                current_char_pos += 1\n",
    "            \n",
    "            # 3. Calculate start/end for the current word\n",
    "            start_idx = current_char_pos\n",
    "            end_idx = current_char_pos + len(word_form)\n",
    "            \n",
    "            # Add the word to the sentence\n",
    "            full_sentence += word_form\n",
    "            current_char_pos += len(word_form)\n",
    "            \n",
    "            # 4. If it is a target, save it with character coordinates\n",
    "            if is_target:\n",
    "                try:\n",
    "                    synset = label_lemma.synset()\n",
    "                    synset_name = synset.name()\n",
    "                    seen_synsets.add(synset_name)\n",
    "                    \n",
    "                    targets_in_sentence.append({\n",
    "                        \"target_word\": word_form,\n",
    "                        \"char_start\": start_idx,\n",
    "                        \"char_end\": end_idx,\n",
    "                        \"label\": synset_name\n",
    "                    })\n",
    "                except:\n",
    "                    # <The problem with lemmas defined not by the Lemma class, \n",
    "                    # but by a regular string, which is why the .synset() method does not work>\n",
    "                    pass\n",
    "\n",
    "        # Save all targets from this sentence\n",
    "        for target in targets_in_sentence:\n",
    "            dataset_rows.append({\n",
    "                \"sentence_id\": sent_idx,\n",
    "                \"sentence\": full_sentence,\n",
    "                \"target_word\": target['target_word'],\n",
    "                \"char_start\": target['char_start'],\n",
    "                \"char_end\": target['char_end'],\n",
    "                \"label\": target['label']\n",
    "            })\n",
    "            \n",
    "        if sent_idx % 2000 == 0 and sent_idx > 0:\n",
    "            print(f\"Processed {sent_idx} sentences...\")\n",
    "\n",
    "    print(f\"Finished. Total examples: {len(dataset_rows)}\")\n",
    "    \n",
    "    # Label Map\n",
    "    sorted_synsets = sorted(list(seen_synsets))\n",
    "    label2id = {label: idx for idx, label in enumerate(sorted_synsets)}\n",
    "    \n",
    "    df = pd.DataFrame(dataset_rows)\n",
    "    df['label_id'] = df['label'].map(label2id)\n",
    "    \n",
    "    print(f\"Saving to {output_file} ...\")\n",
    "    df.to_parquet(output_file, index=False)\n",
    "    \n",
    "    with open(label_map_file, 'w') as f:\n",
    "        print(f\"Saving label map to {label_map_file} ...\")\n",
    "        json.dump(label2id, f)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return df, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04dcab8-039d-4e0e-8d6b-b362d91d4460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset from SemCor corpus...\n",
      "Processed 2000 sentences...\n",
      "Processed 4000 sentences...\n",
      "Processed 6000 sentences...\n",
      "Processed 8000 sentences...\n",
      "Processed 10000 sentences...\n",
      "Processed 12000 sentences...\n",
      "Processed 14000 sentences...\n",
      "Processed 16000 sentences...\n",
      "Processed 18000 sentences...\n",
      "Processed 20000 sentences...\n",
      "Processed 22000 sentences...\n",
      "Processed 24000 sentences...\n",
      "Processed 26000 sentences...\n",
      "Processed 28000 sentences...\n",
      "Processed 30000 sentences...\n",
      "Processed 32000 sentences...\n",
      "Processed 34000 sentences...\n",
      "Processed 36000 sentences...\n",
      "Finished. Total examples: 224716\n",
      "Saving to semcor_train.parquet ...\n",
      "Saving label map to label_map.json ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df, label_map = create_semcor_dataset_with_offsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d14b42-3f95-4fe8-9099-c5024ce11d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>char_start</th>\n",
       "      <th>char_end</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "      <td>Fulton County Grand Jury</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>group.n.01</td>\n",
       "      <td>10742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "      <td>said</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>state.v.01</td>\n",
       "      <td>22071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>friday.n.01</td>\n",
       "      <td>9930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "      <td>investigation</td>\n",
       "      <td>44</td>\n",
       "      <td>57</td>\n",
       "      <td>probe.n.01</td>\n",
       "      <td>18021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>61</td>\n",
       "      <td>68</td>\n",
       "      <td>atlanta.n.01</td>\n",
       "      <td>1529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                                           sentence  \\\n",
       "0            0  The Fulton County Grand Jury said Friday an in...   \n",
       "1            0  The Fulton County Grand Jury said Friday an in...   \n",
       "2            0  The Fulton County Grand Jury said Friday an in...   \n",
       "3            0  The Fulton County Grand Jury said Friday an in...   \n",
       "4            0  The Fulton County Grand Jury said Friday an in...   \n",
       "\n",
       "                target_word  char_start  char_end         label  label_id  \n",
       "0  Fulton County Grand Jury           4        28    group.n.01     10742  \n",
       "1                      said          29        33    state.v.01     22071  \n",
       "2                    Friday          34        40   friday.n.01      9930  \n",
       "3             investigation          44        57    probe.n.01     18021  \n",
       "4                   Atlanta          61        68  atlanta.n.01      1529  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109141c-36e1-41c9-b95e-734ff460d58d",
   "metadata": {},
   "source": [
    "> I'll write a function to check and clean up the dataset, specifically to make sure the indices for the first and last characters of words in the dataset are working right. I'll check both how the indices match the words and how they match the tokens created for the bert-base-uncased model, since this will affect how well the model will be trained later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81ab6c5-fe01-484e-a0de-09d2809102e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_and_clean_dataset(df, output_file=\"semcor_train.parquet\", model_name=\"bert-base-uncased\"):\n",
    "\n",
    "    print(f\"--- Starting verification and cleaning for {model_name} ---\")\n",
    "    print(f\"Initial dataset size: {len(df)}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Counters for statistics\n",
    "    success_count = 0\n",
    "    data_integrity_error_count = 0\n",
    "    content_mismatch_count = 0\n",
    "    token_not_found_count = 0\n",
    "    \n",
    "    # List to store indices of bad rows\n",
    "    indices_to_drop = []\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Iterate through the entire dataframe\n",
    "    for idx in range(total_rows):\n",
    "        row = df.iloc[idx]\n",
    "        sentence = row['sentence']\n",
    "        target_word = row['target_word']\n",
    "        c_start = row['char_start']\n",
    "        c_end = row['char_end']\n",
    "        \n",
    "        # 1. Integrity Check: Does the slice match the target word?\n",
    "        # If not, the coordinates in the dataset are wrong.\n",
    "        slice_check = sentence[c_start:c_end]\n",
    "        if slice_check != target_word:\n",
    "            indices_to_drop.append(idx)\n",
    "            data_integrity_error_count += 1\n",
    "            continue\n",
    "        \n",
    "        # 2. BERT Tokenization Check\n",
    "        # logic: max_length=128 with truncation\n",
    "        encoding = tokenizer(\n",
    "            sentence, \n",
    "            truncation=True, \n",
    "            max_length=128, \n",
    "            return_offsets_mapping=True, \n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        offsets = encoding['offset_mapping']\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "        \n",
    "        found_token_idx = -1\n",
    "        \n",
    "        for i, (o_start, o_end) in enumerate(offsets):\n",
    "            # Skip special tokens ([CLS], [SEP]) which have (0,0) offset\n",
    "            if o_start == 0 and o_end == 0:\n",
    "                continue\n",
    "                \n",
    "            # Strict match: Token starts exactly where the word starts\n",
    "            if o_start == c_start:\n",
    "                found_token_idx = i\n",
    "                break\n",
    "\n",
    "            # Fallback match: Token starts before word but covers the start \n",
    "            # (only if strict match fails)\n",
    "            if o_start < c_start and o_end > c_start:\n",
    "                found_token_idx = i\n",
    "                break\n",
    "        \n",
    "        # 3. Result Evaluation\n",
    "        if found_token_idx != -1:\n",
    "            bert_token = tokens[found_token_idx]\n",
    "            \n",
    "            # Cleanup for comparison\n",
    "            clean_tok = bert_token.replace(\"##\", \"\").lower()\n",
    "            # Handle MWE (Multi-Word Expressions) by taking the first word\n",
    "            clean_tgt = target_word.split()[0].lower() \n",
    "            \n",
    "            # Check if the token is actually the start of our word\n",
    "            if clean_tgt.startswith(clean_tok):\n",
    "                success_count += 1\n",
    "            else:\n",
    "                # Token found, but it doesn't look like the target word\n",
    "                indices_to_drop.append(idx)\n",
    "                content_mismatch_count += 1\n",
    "        else:\n",
    "            # Token not found (likely truncated due to max_length)\n",
    "            indices_to_drop.append(idx)\n",
    "            token_not_found_count += 1\n",
    "\n",
    "        # Progress log\n",
    "        if idx % 10000 == 0 and idx > 0:\n",
    "            print(f\"Processed {idx}/{total_rows} rows...\")\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Verification finished.\")\n",
    "    print(f\"Success: {success_count}\")\n",
    "    print(f\"Integrity Errors: {data_integrity_error_count}\")\n",
    "    print(f\"Content Mismatch: {content_mismatch_count}\")\n",
    "    print(f\"Token Not Found (Truncated): {token_not_found_count}\")\n",
    "    \n",
    "    print(f\"Total rows to drop: {len(indices_to_drop)}\")\n",
    "    \n",
    "    # --- Cleaning ---\n",
    "    if len(indices_to_drop) > 0:\n",
    "        print(f\"Dropping {len(indices_to_drop)} bad rows...\")\n",
    "        \n",
    "        # Drop rows by index\n",
    "        df_clean = df.drop(index=indices_to_drop)\n",
    "        \n",
    "        # Reset index so it is continuous (0, 1, 2...) again\n",
    "        df_clean = df_clean.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"New dataset size: {len(df_clean)}\")\n",
    "        \n",
    "        # Save to Parquet\n",
    "        print(f\"Overwriting {output_file}...\")\n",
    "        df_clean.to_parquet(output_file, index=False)\n",
    "        print(\"Dataset successfully updated.\")\n",
    "        \n",
    "        return df_clean\n",
    "    else:\n",
    "        print(\"No errors found. Dataset is already clean.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbbca471-3994-4fb4-a802-41c97254d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting verification and cleaning for bert-base-uncased ---\n",
      "Initial dataset size: 224716\n",
      "Processed 10000/224716 rows...\n",
      "Processed 20000/224716 rows...\n",
      "Processed 30000/224716 rows...\n",
      "Processed 40000/224716 rows...\n",
      "Processed 50000/224716 rows...\n",
      "Processed 60000/224716 rows...\n",
      "Processed 70000/224716 rows...\n",
      "Processed 80000/224716 rows...\n",
      "Processed 90000/224716 rows...\n",
      "Processed 100000/224716 rows...\n",
      "Processed 110000/224716 rows...\n",
      "Processed 120000/224716 rows...\n",
      "Processed 130000/224716 rows...\n",
      "Processed 140000/224716 rows...\n",
      "Processed 150000/224716 rows...\n",
      "Processed 160000/224716 rows...\n",
      "Processed 170000/224716 rows...\n",
      "Processed 180000/224716 rows...\n",
      "Processed 190000/224716 rows...\n",
      "Processed 200000/224716 rows...\n",
      "Processed 210000/224716 rows...\n",
      "Processed 220000/224716 rows...\n",
      "------------------------------\n",
      "Verification finished.\n",
      "Success: 224515\n",
      "Integrity Errors: 0\n",
      "Content Mismatch: 0\n",
      "Token Not Found (Truncated): 201\n",
      "Total rows to drop: 201\n",
      "Dropping 201 bad rows...\n",
      "New dataset size: 224515\n",
      "Overwriting semcor_train.parquet...\n",
      "Dataset successfully updated.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"semcor_train.parquet\")\n",
    "df_clean = verify_and_clean_dataset(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
